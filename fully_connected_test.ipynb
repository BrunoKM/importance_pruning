{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Critorion Pruned Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this network is to compute the relative importance of each node in a layer of a trained neural network in determining the output over the entire training dataset. This way, the parameters that contribute little (on average) to classification of the output (independently of whether they lead to minimising the cost, unlike a backpropagation algorithm) will get a low importance score. Based on those importance scores, the network can than be pruned, reducing its size. The new network can potentially lead to a better performance, due to higher generalisation of the kept parameters. \n",
    "\n",
    "Below code implements calculations of this criterion on a very simple model: 3 layer fully connected NN on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"Loads mnist data\"\"\"\n",
    "    return input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "mnist = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _weight_variable(shape, random_seed=0):\n",
    "    \"\"\"Create a weight variable with appropriate initialization\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=random_seed)\n",
    "    return tf.Variable(initial, name='weights')\n",
    "\n",
    "\n",
    "def _bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization\"\"\"\n",
    "    initial = tf.constant(0.2, shape=shape)\n",
    "    return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w0 = _weight_variable([784, 800])\n",
    "b0 = _bias_variable([800])\n",
    "x1 = tf.matmul(x0, w0)\n",
    "y1 = tf.nn.relu(x1 + b0)\n",
    "\n",
    "w1 = _weight_variable([800, 800])\n",
    "b1 = _bias_variable([800])\n",
    "x2 = tf.matmul(x1, w1)\n",
    "y2 = tf.nn.relu(x2 + b1)\n",
    "\n",
    "w2 = _weight_variable([800, 10])\n",
    "b2 = _bias_variable([10])\n",
    "x3 = tf.matmul(x2, w2) + b2\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19801, Accuracy: 0.9100000262260437, lr: 0.08999999612569809\r"
     ]
    }
   ],
   "source": [
    "starter_learning_rate = 1e-1\n",
    "lr = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           10000, 0.9, staircase=True)\n",
    "# lr = 0.01\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=x3))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy, global_step=global_step)\n",
    "\n",
    "# Accuracy metrics\n",
    "correct_prediction = tf.equal(tf.argmax(x3, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "try:\n",
    "    for i in range(60000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x0: batch_xs, y_:batch_ys})\n",
    "        if i % 200 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x0: batch_xs, y_: batch_ys})\n",
    "            step, current_lr = sess.run([global_step, lr])\n",
    "            print(\"Step {}, Accuracy: {}, lr: {}\".format(step, train_accuracy, current_lr), end='\\r')\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.8860999941825867\n"
     ]
    }
   ],
   "source": [
    "# Evaluation:\n",
    "\n",
    "baseline_accuracy = accuracy.eval(feed_dict={x0: mnist.test.images,\n",
    "                                             y_: mnist.test.labels})\n",
    "print('Test accuracy {}'.format(baseline_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the weights and biases as numpy arrays\n",
    "w0_save, b0_save, w1_save, b1_save, w2_save, b2_save, last_step = sess.run([w0, b0, w1, b1, w2, b2, global_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eventually load the variables from here to keep on training the model.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w0 = tf.Variable(w0_save)\n",
    "b0 = tf.Variable(b0_save)\n",
    "x1 = tf.matmul(x0, w0)\n",
    "y1 = tf.nn.relu(x1 + b0)\n",
    "\n",
    "w1 = tf.Variable(w1_save)\n",
    "b1 = tf.Variable(b1_save)\n",
    "x2 = tf.matmul(x1, w1)\n",
    "y2 = tf.nn.relu(x2 + b1)\n",
    "\n",
    "w2 = tf.Variable(w2_save)\n",
    "b2 = tf.Variable(b2_save)\n",
    "x3 = tf.matmul(x2, w2) + b2\n",
    "\n",
    "global_step = tf.Variable(last_step, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recreate the graph:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w0 = tf.Variable(w0_save)\n",
    "b0 = tf.Variable(b0_save)\n",
    "x1 = tf.matmul(x0, w0)\n",
    "y1 = tf.nn.relu(x1 + b0)\n",
    "\n",
    "w1 = tf.Variable(w1_save)\n",
    "b1 = tf.Variable(b1_save)\n",
    "x2 = tf.matmul(x1, w1)\n",
    "y2 = tf.nn.relu(x2 + b1)\n",
    "\n",
    "w2 = tf.Variable(w2_save)\n",
    "b2 = tf.Variable(b2_save)\n",
    "x3 = tf.matmul(x2, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_outputs = 10\n",
    "\n",
    "# Define the required asolute values:\n",
    "y0_abs = tf.abs(x0)\n",
    "w0_abs = tf.abs(w0)\n",
    "\n",
    "y1_abs = tf.abs(y1)\n",
    "w1_abs = tf.abs(w1)\n",
    "\n",
    "y2_abs = tf.abs(y2)\n",
    "w2_abs = tf.abs(w2)\n",
    "\n",
    "# Define i3\n",
    "i3 = tf.constant(np.ones([batch_size, n_outputs], dtype=np.float32) / n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the backpropagation of importance\n",
    "i2 = y2_abs * (tf.matmul((i3 / (tf.matmul(y2_abs, w2_abs))), tf.transpose(w2_abs)))\n",
    "\n",
    "i1 = y1_abs * (tf.matmul((i2 / (tf.matmul(y1_abs, w1_abs))), tf.transpose(w1_abs)))\n",
    "\n",
    "i0 = y0_abs * (tf.matmul((i1 / (tf.matmul(y0_abs, w0_abs))), tf.transpose(w0_abs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average the results for the batch\n",
    "i2_batch_avg = tf.reduce_mean(i2, axis=0)\n",
    "i1_batch_avg = tf.reduce_mean(i1, axis=0)\n",
    "i0_batch_avg = tf.reduce_mean(i0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute the importances over the chosen number of batches from the training set while maintaining\n",
    "cumulative moving average.\n",
    "\"\"\"\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Create numpy arrays in which the moving average will be stored:\n",
    "i0_avg, i1_avg, i2_avg = np.zeros([1, 784], dtype=np.float64), np.zeros([1, 800], dtype=np.float64), np.zeros([1, 800], dtype=np.float64)\n",
    "\n",
    "for i in range(600): # 600 batches - iterate over the entire training set\n",
    "    n = i + 1 # The number of elements of the average\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    i0_curr, i1_curr, i2_curr = sess.run([i0_batch_avg, i1_batch_avg, i2_batch_avg],\n",
    "                                         feed_dict={x0: batch_xs, y_:batch_ys})\n",
    "    \n",
    "    # Update the averages:\n",
    "    i2_avg = (i2_avg * i + i2_curr) / (i + 1)\n",
    "    i1_avg = (i1_avg * i + i1_curr) / (i + 1)\n",
    "    i0_avg = (i0_avg * i + i0_curr) / (i + 1)\n",
    "    \n",
    "    if n % 10 == 0:\n",
    "        print(\"Step: {}\".format(n), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average importance of the second layer is: 0.001250\n"
     ]
    }
   ],
   "source": [
    "print(\"The average importance of the second layer is: {:0.6f}\".format(i1_avg.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i0_avg = np.reshape(i0_avg, [784])\n",
    "i1_avg = np.reshape(i1_avg, [800])\n",
    "i2_avg = np.reshape(i2_avg, [800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that would be removed from the second layer (out of 800 total): 229\n"
     ]
    }
   ],
   "source": [
    "removal_thr = 0.5 # Threshold for how small the importance needs to be for the node to be removed.\n",
    "\n",
    "print(\"Number of nodes that would be removed from the second layer (out of 800 total): {}\".format(\n",
    "    (i1_avg < i1_avg.mean()*removal_thr).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1_remove = (i1_avg < i1_avg.mean()*removal_thr).astype(int)\n",
    "l2_remove = (i2_avg < i2_avg.mean()*removal_thr).astype(int)\n",
    "l1_ix = np.argwhere(l1_remove)\n",
    "l2_ix = np.argwhere(l2_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the corresponding weights for:\n",
    "\n",
    "# 1st hidden layer:\n",
    "w0_save_reduced = np.delete(w0_save, l1_ix, axis=1)\n",
    "w1_save_reduced = np.delete(w1_save, l1_ix, axis=0)\n",
    "b0_save_reduced = np.delete(b0_save, l1_ix)\n",
    "\n",
    "# 2nd hidden layer:\n",
    "w1_save_reduced = np.delete(w1_save_reduced, l2_ix, axis=1)\n",
    "w2_save_reduced = np.delete(w2_save, l2_ix, axis=0)\n",
    "b1_save_reduced = np.delete(b1_save, l2_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 571)\n",
      "(571, 499)\n",
      "(499, 10)\n"
     ]
    }
   ],
   "source": [
    "# Print the new size of each layer\n",
    "print(w0_save_reduced.shape)\n",
    "print(w1_save_reduced.shape)\n",
    "print(w2_save_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the thinned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate the graph with pruned parameters:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w0 = tf.Variable(w0_save_reduced)\n",
    "b0 = tf.Variable(b0_save_reduced)\n",
    "x1 = tf.matmul(x0, w0)\n",
    "y1 = tf.nn.relu(x1 + b0)\n",
    "\n",
    "w1 = tf.Variable(w1_save_reduced)\n",
    "b1 = tf.Variable(b1_save_reduced)\n",
    "x2 = tf.matmul(x1, w1)\n",
    "y2 = tf.nn.relu(x2 + b1)\n",
    "\n",
    "w2 = tf.Variable(w2_save_reduced)\n",
    "b2 = tf.Variable(b2_save)\n",
    "x3 = tf.matmul(x2, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "correct_prediction = tf.equal(tf.argmax(x3, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.8860999941825867\n",
      "Pruned accuracy:   0.902400016784668\n"
     ]
    }
   ],
   "source": [
    "# Evaluation:\n",
    "pruned_accuracy = accuracy.eval(feed_dict={x0: mnist.test.images,\n",
    "                                           y_: mnist.test.labels})\n",
    "\n",
    "print('Baseline accuracy: {}'.format(baseline_accuracy))\n",
    "print('Pruned accuracy:   {}'.format(pruned_accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}